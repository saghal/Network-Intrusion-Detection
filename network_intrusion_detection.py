# -*- coding: utf-8 -*-
"""Network Intrusion Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/network-intrusion-detection-cb7f94e7-3ace-43a3-ab2c-cabcc6fa340b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241025/auto/storage/goog4_request%26X-Goog-Date%3D20241025T133353Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc1bd7cefd3dd96af6033b757185d473c6fbbfc3d3dfdadfeadf197b741d62f61fee7f0db981fd30074674301c3aebbfd8afa83588421f6b10a7c46b55d8a7fad40e6d19398fd3f33d7043b03e6cd67ff8a8946a9d7ce7dacf95f9bde160ed777456596c025fef03ca78dded0829596061b505f284c0f1c80e48c4357c14041732826d8e87c6d19eda46ba759e6660367a254de6c461dcf1b56674e04c17ba7e22d571b2c9e19a1856592ccb10ff1d77311104aab14bcff47eb2e7844ba40184d261949c3945cc9621a973a80582a2628215ee3e3b2797801c04f223a395ef10f42fe7d9638f4527913a954e23290c9daedd7b658a3946f9bb8c042266254996f
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""## Install the gdown package
This code cell installs the `gdown` package, which allows downloading files from Google Drive using a command-line interface.

"""

!pip install gdown

"""## Download the UNR-IDD Dataset
This code cell downloads the University of Nevada - Reno Intrusion Detection Dataset (UNR-IDD) from Google Drive using the `gdown` package. This dataset is commonly used for research in intrusion detection systems.

"""

!gdown --id 1ANnT1g7NjbGGOvL1uHDHEQF_Dzx9kK3f

"""## Load and Inspect the UNR-IDD Dataset
This code cell loads the UNR-IDD dataset from a CSV file into a Pandas DataFrame and displays the first few rows. It also counts the occurrences of each category in the 'Label' and 'Binary Label' columns to analyze the distribution of classes in the dataset.
"""

unridd = pd.read_csv('UNR-IDD.csv')

unridd.head()

unridd['Label'].value_counts()

unridd['Binary Label'].value_counts()

"""## Split the Dataset into Training, Validation, and Test Sets
This code cell uses the `train_test_split` function from Scikit-learn to split the UNR-IDD dataset into training, validation, and test sets. It performs a stratified split based on the 'Binary Label' column to ensure that each set has a proportional representation of the classes. The dataset is first divided into training (70%) and a temporary set (30%), which is then split into validation and test sets (50% each).

### Objective

The goal is to classify network traffic into different categories for intrusion detection. We have two classification tasks:
1. **Multi-Class Classification**: Identifying specific types of network behavior or attacks.
   - Classes include:
     - **Normal**: Normal Network Functionality
     - **TCP-SYN**: TCP-SYN Flood
     - **PortScan**: Port Scanning
     - **Overflow**: Flow Table Overflow
     - **Blackhole**: Blackhole Attack
     - **Diversion**: Traffic Diversion Attack

2. **Binary Classification**: Detecting whether network traffic is normal or indicative of an attack.
   - Classes include:
     - **Normal**: Normal Network Functionality
     - **Attack**: Network Intrusion

### Rationale for Separate Data Splits

Given the imbalance in the dataset labels, separate splitting for binary and multi-class classification tasks is preferred for the following reasons:

1. **Preserving Label Distribution**:
   - The `stratify` parameter ensures that each split maintains the original label distribution. By splitting the binary and multi-class datasets separately, we maintain a balanced representation in the training, validation, and test sets for each classification task.

2. **Tailored Handling of Imbalanced Data**:
   - Binary classification has a different level of label imbalance compared to multi-class classification. This separation allows us to implement specific techniques to handle imbalance, such as resampling or class weighting, based on the requirements of each task.

3. **Independent Model Training and Evaluation**:
   - Splitting the datasets separately enables us to design and optimize models specifically for binary and multi-class classification without interference. This independent approach simplifies experimentation and analysis.

4. **Avoiding Data Leakage**:
   - By splitting the datasets for each task independently, we prevent any potential data leakage that could arise from combining the data processing pipelines. This ensures that the test set remains completely unseen by the model during training.

### Implementation

The following steps are taken to split the datasets:

1. **Data Splitting for Multi-Class Classification**:
   - Use `train_test_split` with the `stratify` parameter on the original multi-class labels to create training, validation, and test sets. This ensures the label proportions are preserved across the splits.

2. **Data Splitting for Binary Classification**:
   - Repeat the same process, but this time using the binary labels (`Normal` vs. `Attack`). This ensures that the label distribution for the binary classification problem is also maintained across the splits.

3. **Separate Data Processing Pipelines**:
   - After splitting, preprocess the features separately for the binary and multi-class datasets. This includes encoding categorical features, scaling numerical features, and applying techniques to handle imbalance.

---

By following this strategy, we can build models that are better suited to the specific challenges of both binary and multi-class network intrusion detection tasks.
"""

from sklearn.model_selection import train_test_split

# For stratified split, use 'stratify' parameter with the target column
train_data, temp_data = train_test_split(unridd, test_size=0.3, stratify=unridd['Label'], random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['Label'], random_state=42)

train_data_binary, temp_data_binary = train_test_split(unridd, test_size=0.3, stratify=unridd['Binary Label'], random_state=42)
val_data_binary, test_data_binary = train_test_split(temp_data_binary, test_size=0.5, stratify=temp_data_binary['Binary Label'], random_state=42)

train_data.head()

"""## Prepare Feature and Target Variables
This code cell separates the features and target variables from the training, validation, and test datasets. It drops the 'Label' and 'Binary Label' columns from the feature sets, creating `X_train`, `X_val`, and `X_test`. The corresponding target variables are stored in `y_train`, `y_val`, and `y_test` for multi-class and binary classification tasks.
"""

X_train = train_data.drop(['Label', 'Binary Label'], axis=1)
y_train = train_data['Label']
X_train_binary = train_data_binary.drop(['Label', 'Binary Label'], axis=1)
y_train_binary = train_data_binary['Binary Label']

X_val = val_data.drop(['Label', 'Binary Label'], axis=1)
y_val = val_data['Label']
X_val_binary = val_data_binary.drop(['Label', 'Binary Label'], axis=1)
y_val_binary = val_data_binary['Binary Label']

X_test = test_data.drop(['Label', 'Binary Label'], axis=1)
y_test = test_data['Label']
X_test_binary = test_data_binary.drop(['Label', 'Binary Label'], axis=1)
y_test_binary = test_data_binary['Binary Label']

X_train

"""## Analyze Feature Distributions
This code cell analyzes the distribution of values in specific features of the training dataset. It counts the occurrences of unique values in the 'Port Number', 'Switch ID', and 'is_valid' columns. Additionally, it displays the values of the first row in the feature set `X_train` to provide insight into the data structure.
"""

X_train['Port Number'].value_counts()

X_train['Switch ID'].value_counts()

X_train['is_valid'].value_counts()

X_train.iloc[0]

"""## Encode Categorical Features
This code cell uses the `LabelEncoder` from Scikit-learn to convert categorical features in the training, validation, and test datasets into numerical format. It encodes the 'Switch ID' and 'Port Number' columns for the training set and applies the same transformations to the validation and test sets. After encoding, the original categorical columns are dropped, resulting in `X_train_numeric`, `X_val_numeric`, and `X_test_numeric`, which contain only numerical features for further analysis and model training.

"""

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoders for both features
le_switch = LabelEncoder()
le_port = LabelEncoder()

# Fit and transform the training data for regular labels
X_train['Switch ID Encoded'] = le_switch.fit_transform(X_train['Switch ID'])
X_train['Port Number Encoded'] = le_port.fit_transform(X_train['Port Number'])

# Transform the validation and test data using the fitted LabelEncoders
X_val['Switch ID Encoded'] = le_switch.transform(X_val['Switch ID'])
X_val['Port Number Encoded'] = le_port.transform(X_val['Port Number'])

X_test['Switch ID Encoded'] = le_switch.transform(X_test['Switch ID'])
X_test['Port Number Encoded'] = le_port.transform(X_test['Port Number'])

# Drop the original categorical columns after encoding
X_train_numeric = X_train.drop(['Switch ID', 'Port Number'], axis=1)
X_val_numeric = X_val.drop(['Switch ID', 'Port Number'], axis=1)
X_test_numeric = X_test.drop(['Switch ID', 'Port Number'], axis=1)

# Repeat the encoding process for the binary datasets
X_train_binary['Switch ID Encoded'] = le_switch.fit_transform(X_train_binary['Switch ID'])
X_train_binary['Port Number Encoded'] = le_port.fit_transform(X_train_binary['Port Number'])

X_val_binary['Switch ID Encoded'] = le_switch.transform(X_val_binary['Switch ID'])
X_val_binary['Port Number Encoded'] = le_port.transform(X_val_binary['Port Number'])

X_test_binary['Switch ID Encoded'] = le_switch.transform(X_test_binary['Switch ID'])
X_test_binary['Port Number Encoded'] = le_port.transform(X_test_binary['Port Number'])

# Drop the original categorical columns after encoding in the binary datasets
X_train_numeric_binary = X_train_binary.drop(['Switch ID', 'Port Number'], axis=1)
X_val_numeric_binary = X_val_binary.drop(['Switch ID', 'Port Number'], axis=1)
X_test_numeric_binary = X_test_binary.drop(['Switch ID', 'Port Number'], axis=1)

# Display the encoded training dataset for both versions
print("Encoded Training Dataset (Regular Labels):")
print(X_train_numeric.head())

print("\nEncoded Training Dataset (Binary Labels):")
print(X_train_numeric_binary.head())

X_train_numeric.info()

"""## Inspect Unique Values in Numeric Features
This code cell iterates through each column in the `X_train_numeric` dataset to retrieve and display the unique values along with the count of unique values for each feature. This inspection helps to understand the diversity and distribution of data within the numeric features after encoding.

"""

# Check unique values for the training dataset with regular labels
print("Unique Values in Training Dataset (Regular Labels):")
for column in X_train_numeric.columns:
    unique_values = X_train_numeric[column].unique()  # Get unique values
    num_unique = len(unique_values)  # Number of unique values

    # Print the column name and its unique values
    print(f"\nColumn: {column}")
    print(f"Unique Values: {unique_values}")
    print(f"Number of Unique Values: {num_unique}")

# Check unique values for the training dataset with binary labels
print("\nUnique Values in Training Dataset (Binary Labels):")
for column in X_train_numeric_binary.columns:
    unique_values = X_train_numeric_binary[column].unique()  # Get unique values
    num_unique = len(unique_values)  # Number of unique values

    # Print the column name and its unique values
    print(f"\nColumn: {column}")
    print(f"Unique Values: {unique_values}")
    print(f"Number of Unique Values: {num_unique}")

"""## Drop Columns with a Single Unique Value
This code cell defines a function to identify and drop columns from a DataFrame that contain only a single unique value. The function is then applied to the training, validation, and test datasets (`X_train_numeric`, `X_val_numeric`, and `X_test_numeric`). After executing the function, it prints the names of the columns that were removed from each dataset, which helps in cleaning the data by eliminating redundant features.
"""

# Function to drop columns with a single unique value
def drop_single_value_columns(df):
    # Get columns with only one unique value
    cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]

    # Drop those columns
    df_dropped = df.drop(cols_to_drop, axis=1)

    return df_dropped, cols_to_drop

# Apply the function to training, validation, and test datasets for regular labels
X_train_numeric, dropped_train_cols = drop_single_value_columns(X_train_numeric)
X_val_numeric, dropped_val_cols = drop_single_value_columns(X_val_numeric)
X_test_numeric, dropped_test_cols = drop_single_value_columns(X_test_numeric)

# Apply the function to training, validation, and test datasets for binary labels
X_train_numeric_binary, dropped_train_binary_cols = drop_single_value_columns(X_train_numeric_binary)
X_val_numeric_binary, dropped_val_binary_cols = drop_single_value_columns(X_val_numeric_binary)
X_test_numeric_binary, dropped_test_binary_cols = drop_single_value_columns(X_test_numeric_binary)

# Display the columns that were dropped
print("Dropped columns from training set (Regular Labels):", dropped_train_cols)
print("Dropped columns from validation set (Regular Labels):", dropped_val_cols)
print("Dropped columns from test set (Regular Labels):", dropped_test_cols)

print("\nDropped columns from training set (Binary Labels):", dropped_train_binary_cols)
print("Dropped columns from validation set (Binary Labels):", dropped_val_binary_cols)
print("Dropped columns from test set (Binary Labels):", dropped_test_binary_cols)

X_train_numeric.head()

"""## Visualize Numeric Features with Boxplots
This code cell generates boxplots for each numeric feature in the `X_train_numeric` dataset. It iterates through all numeric columns, creating a boxplot for each to visualize the distribution and identify potential outliers. The plots are displayed one at a time, providing insights into the spread and central tendency of each feature.
"""

import matplotlib.pyplot as plt

# Plot boxplots for each numeric feature
for column in X_train_numeric.select_dtypes(include=['number']).columns:
    plt.figure(figsize=(10, 5))
    plt.title(f'Box Plot for {column}')
    plt.boxplot(X_train_numeric[column].dropna())
    plt.show()

"""## Detect Outliers Using the IQR Method
This code cell defines a function to detect outliers in a DataFrame using the Interquartile Range (IQR) method. It calculates the first (Q1) and third (Q3) quartiles for each numeric column, determines the IQR, and sets the lower and upper bounds for outlier detection. The function identifies indices of the outliers and returns them in a dictionary. Afterward, it applies the function to the `X_train_numeric` dataset and prints the number of outliers found for each column, providing insights into the data quality and potential anomalies.
"""

def detect_outliers_iqr(df):
    outlier_indices = {}

    # Loop through each column in the DataFrame
    for column in df.select_dtypes(include=['number']).columns:
        # Calculate Q1 (25th percentile) and Q3 (75th percentile)
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1

        # Calculate the lower and upper bounds
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Find outliers
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].index
        outlier_indices[column] = list(outliers)

    return outlier_indices

# Detect outliers in the training dataset
outliers = detect_outliers_iqr(X_train_numeric)

# Display outliers for each column
for column, outlier_list in outliers.items():
    print(f"Column: {column}, Number of Outliers: {len(outlier_list)}")

X_train_numeric.describe()

"""## Scale Numeric Features Using Standardization
This code cell utilizes the `StandardScaler` from Scikit-learn to standardize the numeric features in the training, validation, and test datasets. The `fit_transform` method is applied to the training set to compute the mean and standard deviation for scaling, while the `transform` method is used to apply the same scaling parameters to the validation and test sets. The result is stored in `X_train_scaled`, `X_val_scaled`, and `X_test_scaled`, ensuring that the features are centered around zero with a standard deviation of one, which is essential for many machine learning algorithms.
"""

from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Scale the training, validation, and test datasets for regular labels
X_train_scaled = scaler.fit_transform(X_train_numeric)
X_val_scaled = scaler.transform(X_val_numeric)
X_test_scaled = scaler.transform(X_test_numeric)

# Scale the training, validation, and test datasets for binary labels
scaler_binary = StandardScaler()

X_train_scaled_binary = scaler_binary.fit_transform(X_train_numeric_binary)
X_val_scaled_binary = scaler_binary.transform(X_val_numeric_binary)
X_test_scaled_binary = scaler_binary.transform(X_test_numeric_binary)

# Display the scaled training dataset for regular labels
print("Scaled Training Dataset (Regular Labels):")
print(X_train_scaled)

# Display the scaled training dataset for binary labels
print("\nScaled Training Dataset (Binary Labels):")
print(X_train_scaled_binary)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

#Hyperparameter Tuning with GridSearchCV
param_grid = {
    'n_estimators': [100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_scaled, y_train)

# Use the best model from GridSearchCV
best_model = grid_search.best_estimator_
print(best_model.get_params())

# Model Evaluation on the validation set
y_val_pred = best_model.predict(X_val_scaled)
#y_val_pred = model.predict(X_val_scaled)
print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
print("Validation Classification Report:\n", classification_report(y_val, y_val_pred))
print("Validation Confusion Matrix:\n", confusion_matrix(y_val, y_val_pred))

# Testing the model on the test set
y_test_pred = best_model.predict(X_test_scaled)

print("Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("Test Classification Report:\n", classification_report(y_test, y_test_pred))
print("Test Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_numeric.columns)
feature_importances = pd.Series(best_model.feature_importances_, index=X_train_scaled.columns)
feature_importances = feature_importances.sort_values(ascending=False)
print("Feature Importances:\n", feature_importances)

binary_model = RandomForestClassifier(random_state=42)
binary_model.fit(X_train_scaled_binary, y_train_binary)

y_val_binary_pred = binary_model.predict(X_val_scaled_binary)

print("Validation Accuracy:", accuracy_score(y_val_binary, y_val_binary_pred))
print("Validation Classification Report:\n", classification_report(y_val_binary, y_val_binary_pred))
print("Validation Confusion Matrix:\n", confusion_matrix(y_val_binary, y_val_binary_pred))

y_test_binary_pred = binary_model.predict(X_test_scaled_binary)

print("Test Accuracy:", accuracy_score(y_test_binary, y_test_binary_pred))
print("Test Classification Report:\n", classification_report(y_test_binary, y_test_binary_pred))
print("Test Confusion Matrix:\n", confusion_matrix(y_test_binary, y_test_binary_pred))